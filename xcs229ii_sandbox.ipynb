{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xcs229ii_sandbox.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbNchbg22Bod7pVksJFdiH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammarhusain/XCS229ii-project/blob/main/xcs229ii_sandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD_64TSfk_2N"
      },
      "source": [
        "## Ammar's XCS229ii experiments\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG86J6b8rzVA",
        "outputId": "01dd3906-ff59-457d-b2df-eff96138a147"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines3[mpi]\n",
        "#!pip uninstall -y stable-baselines3\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# function to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3[mpi]\n",
            "  Using cached stable_baselines3-1.3.0-py3-none-any.whl (174 kB)\n",
            "\u001b[33mWARNING: stable-baselines3 1.3.0 does not provide the extra 'mpi'\u001b[0m\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[mpi]) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[mpi]) (1.19.5)\n",
            "Requirement already satisfied: gym<0.20,>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[mpi]) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[mpi]) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[mpi]) (1.10.0+cu111)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[mpi]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<0.20,>=0.17->stable-baselines3[mpi]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.20,>=0.17->stable-baselines3[mpi]) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.20,>=0.17->stable-baselines3[mpi]) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->stable-baselines3[mpi]) (3.10.0.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[mpi]) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[mpi]) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[mpi]) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[mpi]) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines3[mpi]) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[mpi]) (2018.9)\n",
            "Installing collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glN6gy1gk_2N",
        "outputId": "5a5414c3-4542-47dd-95a9-6f79bbf565ed"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# separate out some training data to train the RL agent\n",
        "half_data_size = int(len(trainset)/2)\n",
        "\n",
        "rl_agent_trainset = torch.utils.data.Subset(trainset, range(0,int(0.8*half_data_size)))\n",
        "rl_agent_testset = torch.utils.data.Subset(trainset, range(int(0.8*half_data_size), half_data_size))\n",
        "\n",
        "hyp_opt_trainset = torch.utils.data.Subset(trainset, range(0,int(0.8*len(trainset))))\n",
        "hyp_opt_testset = torch.utils.data.Subset(trainset, range(int(0.8*len(trainset)), len(trainset)))\n",
        "\n",
        "print(f\"Full dataset size:  train={len(trainset)} test={len(testset)}\")\n",
        "print(f\"Use a subset of the training data to train the Hyp-RL agent : train={len(rl_agent_trainset)} val={len(rl_agent_testset)}\")\n",
        "\n",
        "print(f\"Use a subset of the training data to compare RL agent against HypOpt baseline  : train={len(hyp_opt_trainset)} val={len(hyp_opt_testset)}\")\n"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Full dataset size:  train=50000 test=10000\n",
            "Use a subset of the training data to train the Hyp-RL agent : train=20000 val=5000\n",
            "Use a subset of the training data to compare RL agent against HypOpt baseline  : train=40000 val=10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGn-0O64lHqh"
      },
      "source": [
        "## function to train and evaluate the model given the hyperparameter setting\n",
        "\n",
        "## define the neural network\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def evaluateFullDataset(hp_learning_rate=0.001):\n",
        "  full_train = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "  full_test = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "  net = Net()\n",
        "  loss_criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr=hp_learning_rate, momentum=0.9)\n",
        "  trainAndEvaluateModel(net, loss_criterion, optimizerm, rl_agent_train, rl_agent_test)\n",
        "\n",
        "def trainAndEvaluateModel(net, loss_criterion, optimizer, train, test):\n",
        "  ## Train the model\n",
        "  for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = net(inputs)\n",
        "          loss = loss_criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "              # print('[%d, %5d] loss: %.3f' %\n",
        "              #       (epoch + 1, i + 1, running_loss / 2000))\n",
        "              running_loss = 0.0\n",
        "  #print('Finished Training')\n",
        "\n",
        "  ## Test the model\n",
        "\n",
        "  # # print images\n",
        "  # dataiter = iter(test)\n",
        "  # images, labels = dataiter.next()\n",
        "  # imshow(torchvision.utils.make_grid(images))\n",
        "  # print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "  # outputs = net(images)\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in test:\n",
        "          images, labels = data\n",
        "          # calculate outputs by running images through the network \n",
        "          outputs = net(images)\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Accuracy of the network on the {len(test)} test images: {(100 * correct / total)}%\")\n",
        "  return (100 * correct / total)\n",
        "  \n"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALe425tvo_mb"
      },
      "source": [
        "## Build the RL environment and agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t4pXmwpo-tG"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "   \n",
        "class TunableHP:\n",
        "  def __init__(self, train_set, eval_set):\n",
        "    self.hyperparameters = {\"learning_rate\":[0.0001, 0.001, 2.0, 0.01, 0.1, 1.0],\n",
        "                            \"batch_size\": [2,4, 9, 6,8]}\n",
        "\n",
        "    #self.hyperparameters = {\"learning_rate\":[-5,-4,-3,-2,-1,0,-1,-2,-3,-4,-5]}\n",
        "    self.hyperparameter_keys = list(self.hyperparameters)\n",
        "\n",
        "    self.train_set = train_set\n",
        "    self.eval_set = eval_set\n",
        "\n",
        "  def mapStateToHP(self,state):\n",
        "    hp_dict = {}\n",
        "    for p,i in enumerate(state):\n",
        "      param_key = self.hyperparameter_keys[p]\n",
        "      hp_dict[param_key] = self.hyperparameters[param_key][i]\n",
        "    return hp_dict\n",
        "  \n",
        "  def getGridSize(self):\n",
        "    return [len(self.hyperparameters[k]) for k in self.hyperparameter_keys]\n",
        "\n",
        "  def evaluateRLAgent(self, hp_dict):\n",
        "    print(f\"Running evaluation for : {hp_dict}\")\n",
        "    return hp_dict['batch_size']*hp_dict['learning_rate']\n",
        "    rl_agent_train = torch.utils.data.DataLoader(self.train_set, batch_size=hp_dict['batch_size'],\n",
        "                                            shuffle=True, num_workers=2)\n",
        "    rl_agent_test = torch.utils.data.DataLoader(self.eval_set, batch_size=hp_dict['batch_size'],\n",
        "                                          shuffle=False, num_workers=2)\n",
        "    net = Net()\n",
        "    loss_criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=hp_dict['learning_rate'], momentum=0.9)\n",
        "    return trainAndEvaluateModel(net, loss_criterion, optimizer, rl_agent_train, rl_agent_test)\n",
        "\n",
        "class HypRLGridEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface.\n",
        "  This is a simple env where the agent must learn to go always left. \n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  MAX_ITER = 10\n",
        "\n",
        "  def __init__(self, tunableParams=TunableHP(rl_agent_trainset, rl_agent_testset)):\n",
        "    super(HypRLGridEnv, self).__init__()\n",
        "\n",
        "    self.tunableParams = tunableParams\n",
        "\n",
        "    # Size of the grid\n",
        "    self.grid_size = tunableParams.getGridSize()\n",
        "    \n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions, we have two: left and right\n",
        "    n_actions = 3\n",
        "    self.action_space = spaces.Box(low=-1, high=1, shape=(len(self.grid_size),), dtype=np.int32)\n",
        "    # The observation will be the coordinate of the agent\n",
        "    # this can be described both by Discrete and Box space\n",
        "    self.observation_space = spaces.MultiDiscrete(self.grid_size)\n",
        "    self.eval_cache = np.zeros(self.grid_size)\n",
        "\n",
        "  def eval(self, state):\n",
        "    state = tuple(state)\n",
        "    if self.eval_cache[state] == [0.0]:\n",
        "      # train & test the model for these hyperparameters\n",
        "      self.eval_cache[state] = self.tunableParams.evaluateRLAgent(self.tunableParams.mapStateToHP(state))\n",
        "    return self.eval_cache[state]\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # reset the number of iterations for this agent\n",
        "    self.iter = 0\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_state = np.random.randint(self.grid_size)\n",
        "    self.reward = self.eval(self.agent_state)\n",
        "    self.visited = {}\n",
        "    self.visited[tuple(self.agent_state)] = True\n",
        "    return np.array(self.agent_state) \n",
        "\n",
        "  def step(self, action):\n",
        "    self.iter += 1\n",
        "\n",
        "    for i, _ in enumerate(action):\n",
        "      self.agent_state[i] += action[i]\n",
        "      # Account for the boundaries of the grid\n",
        "      self.agent_state[i] = np.clip(self.agent_state[i], 0, self.grid_size[i]-1)\n",
        "\n",
        "    # We are done when we visit the same state twice or have taken more iterations than MAX\n",
        "    done = bool(self.iter >= self.MAX_ITER or tuple(self.agent_state) in self.visited)\n",
        "\n",
        "    self.visited[tuple(self.agent_state)] = True\n",
        "\n",
        "    # Reward is minimum of whatever val loss we saw so far\n",
        "    self.reward = max(self.reward, self.eval(self.agent_state))\n",
        "\n",
        "    # Null reward everywhere except when the episode terminates\n",
        "    reward = self.reward if done else 0\n",
        "\n",
        "    # Optionally we can pass additional info, we are not using that for now\n",
        "    info = {}\n",
        "    return np.array(self.agent_state), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # agent is represented as a cross, rest as a dot\n",
        "    print(\".\" * self.agent_state, end=\"\")\n",
        "    print(\"x\", end=\"\")\n",
        "    print(\".\" * (self.grid_size - self.agent_state))\n",
        "\n",
        "  def close(self):\n",
        "    pass\n",
        "\n",
        "# check and make sure the environment is sane and working\n",
        "#from stable_baselines.common.env_checker import check_env\n",
        "\n",
        "#env = HypRLGridEnv()\n",
        "# If the environment doesn't follow the interface, an error will be thrown\n",
        "#check_env(env, warn=True)\n",
        "#env.render()"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnOq39NCtzbi"
      },
      "source": [
        "### RL Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_gxuc5NtwcW",
        "outputId": "7e1c5a3a-b17b-4023-bb80-4a884f153b2e"
      },
      "source": [
        "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
        "from stable_baselines.common.cmd_util import make_vec_env\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "import pdb\n",
        "# # Instantiate the env\n",
        "env = HypRLGridEnv()\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Train the agent\n",
        "##model = ACKTR('MlpPolicy', env, verbose=1).learn(5000)\n",
        "model = A2C(MlpPolicy, env, verbose=0)\n",
        "model.learn(total_timesteps=25000)\n"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.grid_size 2 n_actions*np.ones(self.grid_size) [3. 3.]\n",
            "self.action_space.sample() [0 1]\n",
            "self.observation_space.sample() [4 0]\n",
            "Running evaluation for : {'learning_rate': 0.01, 'batch_size': 9}\n",
            "Running evaluation for : {'learning_rate': 0.01, 'batch_size': 4}\n",
            "Running evaluation for : {'learning_rate': 2.0, 'batch_size': 4}\n",
            "Running evaluation for : {'learning_rate': 0.001, 'batch_size': 2}\n",
            "Running evaluation for : {'learning_rate': 0.0001, 'batch_size': 2}\n",
            "Running evaluation for : {'learning_rate': 0.01, 'batch_size': 2}\n",
            "Running evaluation for : {'learning_rate': 0.001, 'batch_size': 9}\n",
            "Running evaluation for : {'learning_rate': 2.0, 'batch_size': 9}\n",
            "Running evaluation for : {'learning_rate': 0.001, 'batch_size': 6}\n",
            "Running evaluation for : {'learning_rate': 0.0001, 'batch_size': 6}\n",
            "Running evaluation for : {'learning_rate': 0.001, 'batch_size': 8}\n",
            "Running evaluation for : {'learning_rate': 1.0, 'batch_size': 9}\n",
            "Running evaluation for : {'learning_rate': 1.0, 'batch_size': 6}\n",
            "Running evaluation for : {'learning_rate': 0.1, 'batch_size': 9}\n",
            "Running evaluation for : {'learning_rate': 1.0, 'batch_size': 8}\n",
            "Running evaluation for : {'learning_rate': 0.1, 'batch_size': 8}\n",
            "Running evaluation for : {'learning_rate': 0.01, 'batch_size': 8}\n",
            "Running evaluation for : {'learning_rate': 0.01, 'batch_size': 6}\n",
            "Running evaluation for : {'learning_rate': 2.0, 'batch_size': 2}\n",
            "Running evaluation for : {'learning_rate': 0.1, 'batch_size': 2}\n",
            "Running evaluation for : {'learning_rate': 0.001, 'batch_size': 4}\n",
            "Running evaluation for : {'learning_rate': 0.0001, 'batch_size': 4}\n",
            "Running evaluation for : {'learning_rate': 2.0, 'batch_size': 6}\n",
            "Running evaluation for : {'learning_rate': 0.1, 'batch_size': 6}\n",
            "Running evaluation for : {'learning_rate': 2.0, 'batch_size': 8}\n",
            "Running evaluation for : {'learning_rate': 0.0001, 'batch_size': 9}\n",
            "Running evaluation for : {'learning_rate': 1.0, 'batch_size': 4}\n",
            "Running evaluation for : {'learning_rate': 0.1, 'batch_size': 4}\n",
            "Running evaluation for : {'learning_rate': 0.0001, 'batch_size': 8}\n",
            "Running evaluation for : {'learning_rate': 1.0, 'batch_size': 2}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines.a2c.a2c.A2C at 0x7f06d9536990>"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb0mvA28dKRz",
        "outputId": "cd9be469-5ead-40b4-8bb3-199454d8e7ce"
      },
      "source": [
        "# Test the trained agent for sanity checking on the same environment\n",
        "obs = env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  #pdb.set_trace()\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward, \"final_state=\", info[0]['terminal_observation'])\n",
        "    break\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  #env.render(mode='console')\n",
        "\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "print(f\"{env.envs[0].eval_cache}\")"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Action:  [[1.000 1.000]]\n",
            "obs= [[2 4]] reward= [0.000] done= [False]\n",
            "Step 2\n",
            "Action:  [[1.000 1.000]]\n",
            "obs= [[3 4]] reward= [0.000] done= [False]\n",
            "Step 3\n",
            "Action:  [[1.000 1.000]]\n",
            "obs= [[4 4]] reward= [0.000] done= [False]\n",
            "Step 4\n",
            "Action:  [[1.000 1.000]]\n",
            "obs= [[5 4]] reward= [0.000] done= [False]\n",
            "Step 5\n",
            "Action:  [[1.000 1.000]]\n",
            "Goal reached! reward= [16.000] final_state= [5 4]\n",
            "[[0.000 0.000 0.001 0.001 0.001]\n",
            " [0.002 0.004 0.009 0.006 0.008]\n",
            " [4.000 8.000 18.000 12.000 16.000]\n",
            " [0.020 0.040 0.090 0.060 0.080]\n",
            " [0.200 0.400 0.900 0.600 0.800]\n",
            " [2.000 4.000 9.000 6.000 8.000]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oEd0rDpAYHI"
      },
      "source": [
        "## Time to perform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUKMx8hSuUTx"
      },
      "source": [
        "# Instantiate a full environment\n",
        "env_real = HypRLGridEnv(TunableHP(hyp_opt_trainset, hyp_opt_trainset))\n",
        "# wrap it\n",
        "env_real = make_vec_env(lambda: env_real, n_envs=1)\n",
        "\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CsUbWSWDyPH",
        "outputId": "b18adc69-78ed-411e-bb8c-c91a134addaa"
      },
      "source": [
        "# Test the trained agent on a new and full environment of the same dataset\n",
        "obs = env_real.reset()\n",
        "print('obs=', obs)\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  #pdb.set_trace()\n",
        "  obs, reward, done, info = env_real.step(action)\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward, \"final_state=\", info[0]['terminal_observation'])\n",
        "    break\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "print(env_real.envs[0].eval_cache)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obs= [[0]]\n",
            "Step 1\n",
            "Action:  [1]\n",
            "obs= [[1]] reward= [0.] done= [False]\n",
            "Step 2\n",
            "Action:  [2]\n",
            "Goal reached! reward= [51.0225] final_state= [1]\n",
            "[16.3575 51.0225 43.9325 10.0575  9.965 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR8AqJ35cHtA",
        "outputId": "711f8534-c88f-43e0-c5e1-2f0f7e56c82f"
      },
      "source": [
        "print(env_real.envs[0].eval_cache)\n",
        "obs = env_real.reset()\n",
        "print('obs=', obs)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.     51.0225 43.9325 10.0575  9.965 ]\n",
            "obs= [[3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEnq60GncQSz"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "kfold.split(trainset)\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(trainset)):\n",
        "  print(f\"fold {fold} ... train {len(train_ids)} ... test {len(test_ids)}\")\n",
        "\n",
        "kfold.get_n_splits()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}