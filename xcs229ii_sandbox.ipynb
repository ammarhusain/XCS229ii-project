{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xcs229ii_sandbox.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhcKQrWT/pCkzyvWRvbs3S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammarhusain/XCS229ii-project/blob/master/XCS229ii-project/xcs229ii_sandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD_64TSfk_2N"
      },
      "source": [
        "## Ammar's XCS229ii experiments\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG86J6b8rzVA",
        "outputId": "4b326d46-7585-43bc-db45-f789efef252c"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# function to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting stable-baselines[mpi]==2.10.0\n",
            "  Downloading stable_baselines-2.10.0-py3-none-any.whl (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.5)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: mpi4py in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (3.0.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
            "Installing collected packages: stable-baselines\n",
            "  Attempting uninstall: stable-baselines\n",
            "    Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glN6gy1gk_2N",
        "outputId": "d550f939-4f77-453f-84fc-da44fc1b227d"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# separate out some training data to train the RL agent\n",
        "half_data_size = int(len(trainset)/2)\n",
        "\n",
        "rl_agent_trainset = torch.utils.data.Subset(trainset, range(0,int(0.8*half_data_size)))\n",
        "rl_agent_testset = torch.utils.data.Subset(trainset, range(int(0.8*half_data_size), half_data_size))\n",
        "\n",
        "hyp_opt_trainset = torch.utils.data.Subset(trainset, range(0,int(0.8*len(trainset))))\n",
        "hyp_opt_testset = torch.utils.data.Subset(trainset, range(int(0.8*len(trainset)), len(trainset)))\n",
        "\n",
        "print(f\"Full dataset size:  train={len(trainset)} test={len(testset)}\")\n",
        "print(f\"Use a subset of the training data to train the Hyp-RL agent : train={len(rl_agent_trainset)} val={len(rl_agent_testset)}\")\n",
        "\n",
        "print(f\"Use a subset of the training data to compare RL agent against HypOpt baseline  : train={len(hyp_opt_trainset)} val={len(hyp_opt_testset)}\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Full dataset size:  train=50000 test=10000\n",
            "Use a subset of the training data to train the Hyp-RL agent : train=20000 val=5000\n",
            "Use a subset of the training data to compare RL agent against HypOpt baseline  : train=40000 val=10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGn-0O64lHqh"
      },
      "source": [
        "## function to train and evaluate the model given the hyperparameter setting\n",
        "\n",
        "## define the neural network\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def evaluateFullDataset(hp_learning_rate=0.001):\n",
        "  full_train = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "  full_test = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "  net = Net()\n",
        "  loss_criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr=hp_learning_rate, momentum=0.9)\n",
        "  trainAndEvaluateModel(net, loss_criterion, optimizerm, rl_agent_train, rl_agent_test)\n",
        "\n",
        "def trainAndEvaluateModel(net, loss_criterion, optimizer, train, test):\n",
        "  ## Train the model\n",
        "  for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = net(inputs)\n",
        "          loss = loss_criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "              # print('[%d, %5d] loss: %.3f' %\n",
        "              #       (epoch + 1, i + 1, running_loss / 2000))\n",
        "              running_loss = 0.0\n",
        "  #print('Finished Training')\n",
        "\n",
        "  ## Test the model\n",
        "\n",
        "  # # print images\n",
        "  # dataiter = iter(test)\n",
        "  # images, labels = dataiter.next()\n",
        "  # imshow(torchvision.utils.make_grid(images))\n",
        "  # print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "  # outputs = net(images)\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in test:\n",
        "          images, labels = data\n",
        "          # calculate outputs by running images through the network \n",
        "          outputs = net(images)\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Accuracy of the network on the {len(test)} test images: {(100 * correct / total)}%\")\n",
        "  return (100 * correct / total)\n",
        "  \n",
        "\n",
        "#evaluateFullDataset()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALe425tvo_mb"
      },
      "source": [
        "## Build the RL environment and agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t4pXmwpo-tG"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "   \n",
        "class TunableHP:\n",
        "  def __init__(self, train_set, eval_set):\n",
        "    self.hyperparameters = {\"learning_rate\":[0.0001, 0.001, 0.01, 0.1, 1.0]}\n",
        "\n",
        "    #self.hyperparameters = {\"learning_rate\":[-5,-4,-3,-2,-1,0,-1,-2,-3,-4,-5]}\n",
        "    self.hyperparameter_keys = list(self.hyperparameters)\n",
        "\n",
        "    self.train_set = train_set\n",
        "    self.eval_set = eval_set\n",
        "\n",
        "  def mapStateToHP(self,state):\n",
        "    return [self.hyperparameters[self.hyperparameter_keys[p]][i] for p,i in enumerate(state)]\n",
        "  \n",
        "  def getGridSize(self):\n",
        "    return [len(self.hyperparameters[k]) for k in self.hyperparameter_keys]\n",
        "\n",
        "  def evaluateRLAgent(self, hp_learning_rate):\n",
        "    print(f\"Running evaluation for : {hp_learning_rate}\")\n",
        "    #return hp_learning_rate\n",
        "    rl_agent_train = torch.utils.data.DataLoader(self.train_set, batch_size=batch_size,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "    rl_agent_test = torch.utils.data.DataLoader(self.eval_set, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "    net = Net()\n",
        "    loss_criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=hp_learning_rate, momentum=0.9)\n",
        "    return trainAndEvaluateModel(net, loss_criterion, optimizer, rl_agent_train, rl_agent_test)\n",
        "\n",
        "\n",
        "class HypRLGridEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface.\n",
        "  This is a simple env where the agent must learn to go always left. \n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Define constants for clearer code\n",
        "  UP = 0\n",
        "  DOWN = 1\n",
        "  STAY = 2\n",
        "  MAX_ITER = 10\n",
        "\n",
        "  def __init__(self, tunableParams=TunableHP(rl_agent_trainset, rl_agent_testset)):\n",
        "    super(HypRLGridEnv, self).__init__()\n",
        "\n",
        "    self.tunableParams = tunableParams\n",
        "\n",
        "    # Size of the grid\n",
        "    self.grid_size = tunableParams.getGridSize()\n",
        "    \n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions, we have two: left and right\n",
        "    n_actions = 3\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    # The observation will be the coordinate of the agent\n",
        "    # this can be described both by Discrete and Box space\n",
        "    self.observation_space = spaces.MultiDiscrete(self.grid_size)\n",
        "\n",
        "    self.eval_cache = np.zeros(self.grid_size)\n",
        "\n",
        "  def eval(self, state):\n",
        "    state = tuple(state)\n",
        "    if self.eval_cache[state] == [0.0]:\n",
        "      # train & test the model for these hyperparameters\n",
        "      self.eval_cache[state] = self.tunableParams.evaluateRLAgent(*self.tunableParams.mapStateToHP(state))\n",
        "    return self.eval_cache[state]\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # reset the number of iterations for this agent\n",
        "    self.iter = 0\n",
        "    self.reward = -np.inf\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_state = np.random.randint(self.grid_size)\n",
        "\n",
        "    start_state = self.agent_state\n",
        "    self.visited = {}\n",
        "    return np.array(start_state) \n",
        "\n",
        "  def step(self, action):\n",
        "    self.iter += 1\n",
        "\n",
        "    if action == self.UP:\n",
        "      self.agent_state -= 1\n",
        "    elif action == self.DOWN:\n",
        "      self.agent_state += 1\n",
        "    elif action == self.STAY:\n",
        "      self.agent_state = self.agent_state      \n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "    # Account for the boundaries of the grid\n",
        "    for i, _ in enumerate(self.agent_state):\n",
        "      self.agent_state[i] = np.clip(self.agent_state[i], 0, self.grid_size[i]-1)\n",
        "\n",
        "    # We are done when we visit the same state twice or have taken more iterations than MAX\n",
        "    done = bool(self.iter >= self.MAX_ITER or tuple(self.agent_state) in self.visited)\n",
        "\n",
        "    self.visited[tuple(self.agent_state)] = True\n",
        "\n",
        "    # Reward is minimum of whatever val loss we saw so far\n",
        "    self.reward = max(self.reward, self.eval(self.agent_state))\n",
        "\n",
        "    # Null reward everywhere except when the episode terminates\n",
        "    reward = self.reward if done else 0\n",
        "\n",
        "    # Optionally we can pass additional info, we are not using that for now\n",
        "    info = {}\n",
        "    return np.array(self.agent_state), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # agent is represented as a cross, rest as a dot\n",
        "    print(\".\" * self.agent_state, end=\"\")\n",
        "    print(\"x\", end=\"\")\n",
        "    print(\".\" * (self.grid_size - self.agent_state))\n",
        "\n",
        "  def close(self):\n",
        "    pass\n",
        "\n",
        "# check and make sure the environment is sane and working\n",
        "from stable_baselines.common.env_checker import check_env\n",
        "\n",
        "#env = HypRLGridEnv()\n",
        "# If the environment doesn't follow the interface, an error will be thrown\n",
        "#check_env(env, warn=True)\n",
        "#env.render()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnOq39NCtzbi"
      },
      "source": [
        "### RL Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_gxuc5NtwcW",
        "outputId": "692153f9-baa0-408e-fe27-cf07aad4e865"
      },
      "source": [
        "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
        "from stable_baselines.common.cmd_util import make_vec_env\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "import pdb\n",
        "# Instantiate the env\n",
        "env = HypRLGridEnv()\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Train the agent\n",
        "##model = ACKTR('MlpPolicy', env, verbose=1).learn(5000)\n",
        "model = A2C(MlpPolicy, env, verbose=0)\n",
        "model.learn(total_timesteps=25000)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running evaluation for : 1.0\n",
            "Accuracy of the network on the 625 test images: 9.72%\n",
            "Running evaluation for : 0.0001\n",
            "Accuracy of the network on the 625 test images: 10.38%\n",
            "Running evaluation for : 0.001\n",
            "Accuracy of the network on the 625 test images: 41.58%\n",
            "Running evaluation for : 0.01\n",
            "Accuracy of the network on the 625 test images: 36.12%\n",
            "Running evaluation for : 0.1\n",
            "Accuracy of the network on the 625 test images: 9.62%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines.a2c.a2c.A2C at 0x7f06f291c190>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb0mvA28dKRz",
        "outputId": "449efe6d-8a48-4b99-c243-b323857e6bb8"
      },
      "source": [
        "# Test the trained agent for sanity checking on the same environment\n",
        "obs = env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  #pdb.set_trace()\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward, \"final_state=\", info[0]['terminal_observation'])\n",
        "    break\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  #env.render(mode='console')\n",
        "print(env.envs[0].eval_cache)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Action:  [2]\n",
            "obs= [[1]] reward= [0.] done= [False]\n",
            "Step 2\n",
            "Action:  [2]\n",
            "Goal reached! reward= [41.58] final_state= [1]\n",
            "[10.38 41.58 36.12  9.62  9.72]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oEd0rDpAYHI"
      },
      "source": [
        "## Time to perform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUKMx8hSuUTx"
      },
      "source": [
        "# Instantiate a full environment\n",
        "env_real = HypRLGridEnv(TunableHP(hyp_opt_trainset, hyp_opt_trainset))\n",
        "# wrap it\n",
        "env_real = make_vec_env(lambda: env_real, n_envs=1)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CsUbWSWDyPH",
        "outputId": "8445eed8-b759-416e-db30-09f6635bb075"
      },
      "source": [
        "# Test the trained agent on a new and full environment of the same dataset\n",
        "obs = env_real.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  #pdb.set_trace()\n",
        "  obs, reward, done, info = env_real.step(action)\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward, \"final_state=\", info[0]['terminal_observation'])\n",
        "    break\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "print(env_real.envs[0].eval_cache)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Action:  [0]\n",
            "Running evaluation for : 0.01\n",
            "Accuracy of the network on the 5000 test images: 43.26%\n",
            "obs= [[2]] reward= [0.] done= [False]\n",
            "Step 2\n",
            "Action:  [0]\n",
            "Running evaluation for : 0.001\n",
            "Accuracy of the network on the 5000 test images: 50.0275%\n",
            "obs= [[1]] reward= [0.] done= [False]\n",
            "Step 3\n",
            "Action:  [2]\n",
            "Goal reached! reward= [50.0275] final_state= [1]\n",
            "[ 0.     50.0275 43.26    0.      0.    ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR8AqJ35cHtA",
        "outputId": "7326790e-7089-4202-8dd3-9542c4d150a6"
      },
      "source": [
        "print(env.envs[0].eval_cache)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_get_indices', '_get_target_envs', '_obs_from_buf', '_save_obs', 'action_space', 'actions', 'buf_dones', 'buf_infos', 'buf_obs', 'buf_rews', 'close', 'env_method', 'envs', 'get_attr', 'get_images', 'getattr_depth_check', 'keys', 'metadata', 'num_envs', 'observation_space', 'render', 'reset', 'seed', 'set_attr', 'step', 'step_async', 'step_wait', 'unwrapped']\n",
            "[13.68 41.24  0.    0.    0.  ]\n"
          ]
        }
      ]
    }
  ]
}