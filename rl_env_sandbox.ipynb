{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_env_sandbox.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHNba7ZIU1EXlArHRB8bg1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammarhusain/XCS229ii-project/blob/main/rl_env_sandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLP8di6EwYmQ"
      },
      "source": [
        "## Custom gym environment\n",
        "\n",
        "- Sets up a custom & barebones OpenAI gym environment \n",
        "- Uses an off the shelf RL algorithm from stable baselines 3 to plugin and solve a simple grid search problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRRzDHLjxYeo",
        "outputId": "47f16094-d45b-4519-c5ab-a19b828713bb"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting stable-baselines[mpi]==2.10.0\n",
            "  Downloading stable_baselines-2.10.0-py3-none-any.whl (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 16.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: mpi4py in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (3.0.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
            "Installing collected packages: stable-baselines\n",
            "  Attempting uninstall: stable-baselines\n",
            "    Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAc9FY14xO-H",
        "outputId": "e93639ca-0c9f-4344-9b88-f6c0622264fc"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "class HypRLGridEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface.\n",
        "  This is a simple env where the agent must learn to go always left. \n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Define constants for clearer code\n",
        "  UP = 0\n",
        "  DOWN = 1\n",
        "  MAX_ITER = 10\n",
        "  # second dimenstion on how the agent is progressing\n",
        "  LOWER=0\n",
        "  SAME=1\n",
        "  HIGHER=2\n",
        "  ## hardcoded ground truth reward\n",
        "  GROUND_TRUTH = [-5,-4,-3,-2,-1,0,-1,-2,-3,-4,-5]\n",
        "  def __init__(self, grid_size=11):\n",
        "    super(HypRLGridEnv, self).__init__()\n",
        "\n",
        "    # Size of the 1D-grid\n",
        "    self.grid_size = grid_size\n",
        "    self.grid_shape = [grid_size, 3]\n",
        "    # Initialize the agent randomly\n",
        "    self.agent_pos = np.random.randint(grid_size)\n",
        "\n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions, we have two: left and right\n",
        "    n_actions = 2\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    # The observation will be the coordinate of the agent\n",
        "    # this can be described both by Discrete and Box space\n",
        "    self.observation_space = spaces.MultiDiscrete([grid_size, 3])\n",
        "\n",
        "    #self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # reset the number of iterations for this agent\n",
        "    self.iter = 0\n",
        "    self.reward = -np.inf\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_pos = np.random.randint(self.grid_size)\n",
        "\n",
        "    start_state = [self.agent_pos, 1]\n",
        "    self.visited = [start_state]\n",
        "    return np.array(start_state) # same bucket\n",
        "\n",
        "  def step(self, action):\n",
        "    self.iter += 1\n",
        "    prev_grid_val =  self.GROUND_TRUTH[self.agent_pos]\n",
        "    curr_pos = self.agent_pos\n",
        "    if action == self.UP:\n",
        "      self.agent_pos -= 1\n",
        "    elif action == self.DOWN:\n",
        "      self.agent_pos += 1\n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "    # Account for the boundaries of the grid\n",
        "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size-1)\n",
        "\n",
        "    curr_grid_val =  self.GROUND_TRUTH[self.agent_pos]\n",
        "    # Figure out where the agent falls in the high,same,low bucket\n",
        "    high_low_bucket = self.SAME\n",
        "    if prev_grid_val < curr_grid_val:\n",
        "      high_low_bucket = self.HIGHER\n",
        "    elif prev_grid_val > curr_grid_val:\n",
        "      high_low_bucket = self.LOWER\n",
        "\n",
        "    # set the new state\n",
        "    new_state = [self.agent_pos, high_low_bucket]\n",
        "\n",
        "    # We are done when we visit the same state twice or have taken more iterations than MAX\n",
        "    done = bool(self.iter >= self.MAX_ITER or new_state in self.visited)\n",
        "\n",
        "    self.visited.append(new_state)\n",
        "\n",
        "    # Reward is minimum of whatever val loss we saw so far\n",
        "    self.reward = max(self.reward, curr_grid_val)\n",
        "\n",
        "    # Null reward everywhere except when the episode terminates\n",
        "    reward = self.reward if done else 0\n",
        "\n",
        "    # Optionally we can pass additional info, we are not using that for now\n",
        "    info = {}\n",
        "    return np.array(new_state), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # agent is represented as a cross, rest as a dot\n",
        "    print(\".\" * self.agent_pos, end=\"\")\n",
        "    print(\"x\", end=\"\")\n",
        "    print(\".\" * (self.grid_size - self.agent_pos))\n",
        "\n",
        "  def close(self):\n",
        "    pass\n",
        "\n",
        "# check and make sure the environment is sane and working\n",
        "from stable_baselines.common.env_checker import check_env\n",
        "\n",
        "env = HypRLGridEnv()\n",
        "# If the environment doesn't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            ".....x......\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37thSqUgxR56",
        "outputId": "7b0ac45c-a1fb-43b6-bb18-1de4c660cf8b"
      },
      "source": [
        "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
        "from stable_baselines.common.cmd_util import make_vec_env\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "import pdb\n",
        "# Instantiate the env\n",
        "env = HypRLGridEnv()\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Train the agent\n",
        "#model = ACKTR('MlpPolicy', env, verbose=1).learn(5000)\n",
        "model = A2C(MlpPolicy, env, verbose=1)\n",
        "model.learn(total_timesteps=25000)\n",
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  #pdb.set_trace()\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(obs, ' info=', info)\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward, \"final_state=\", info[0]['terminal_observation'])\n",
        "    break\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render(mode='console')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/input.py:42: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/a2c/a2c.py:158: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/a2c/a2c.py:182: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/a2c/a2c.py:192: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/a2c/a2c.py:194: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "---------------------------------\n",
            "| ep_len_mean        | 3        |\n",
            "| ep_reward_mean     | -2       |\n",
            "| explained_variance | 0.158    |\n",
            "| fps                | 12       |\n",
            "| nupdates           | 1        |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 5        |\n",
            "| value_loss         | 2.01     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.49     |\n",
            "| ep_reward_mean     | -1.93    |\n",
            "| explained_variance | -0.25    |\n",
            "| fps                | 486      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 1.96     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.27     |\n",
            "| ep_reward_mean     | -2       |\n",
            "| explained_variance | 0.442    |\n",
            "| fps                | 606      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 2.05     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.56     |\n",
            "| ep_reward_mean     | -2.16    |\n",
            "| explained_variance | -0.186   |\n",
            "| fps                | 675      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 8.4      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.87     |\n",
            "| ep_reward_mean     | -1.59    |\n",
            "| explained_variance | -2.38    |\n",
            "| fps                | 706      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 0.46     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.38     |\n",
            "| ep_reward_mean     | -2.03    |\n",
            "| explained_variance | 0.0231   |\n",
            "| fps                | 725      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 3.2      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.38     |\n",
            "| ep_reward_mean     | -1.95    |\n",
            "| explained_variance | -0.787   |\n",
            "| fps                | 742      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 0.278    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.27     |\n",
            "| ep_reward_mean     | -2.03    |\n",
            "| explained_variance | -4.32    |\n",
            "| fps                | 748      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 0.693    |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 2.98     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.18     |\n",
            "| ep_reward_mean     | -2.39    |\n",
            "| explained_variance | 0.539    |\n",
            "| fps                | 759      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 0.692    |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 0.48     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.27     |\n",
            "| ep_reward_mean     | -1.94    |\n",
            "| explained_variance | 0.378    |\n",
            "| fps                | 768      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 0.688    |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 1.77     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| ep_len_mean        | 4.41      |\n",
            "| ep_reward_mean     | -1.67     |\n",
            "| explained_variance | -7.76e+03 |\n",
            "| fps                | 774       |\n",
            "| nupdates           | 1000      |\n",
            "| policy_entropy     | 0.663     |\n",
            "| total_timesteps    | 5000      |\n",
            "| value_loss         | 0.691     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.59     |\n",
            "| ep_reward_mean     | -1.4     |\n",
            "| explained_variance | -3.5     |\n",
            "| fps                | 779      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 0.57     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 0.719    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.62     |\n",
            "| ep_reward_mean     | -0.97    |\n",
            "| explained_variance | 0.916    |\n",
            "| fps                | 784      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 0.53     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 0.0529   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| ep_len_mean        | 4.6       |\n",
            "| ep_reward_mean     | -0.66     |\n",
            "| explained_variance | -4.29e+04 |\n",
            "| fps                | 789       |\n",
            "| nupdates           | 1300      |\n",
            "| policy_entropy     | 0.381     |\n",
            "| total_timesteps    | 6500      |\n",
            "| value_loss         | 0.0218    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.74     |\n",
            "| ep_reward_mean     | -0.39    |\n",
            "| explained_variance | nan      |\n",
            "| fps                | 793      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 0.37     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 0.00841  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| ep_len_mean        | 4.8       |\n",
            "| ep_reward_mean     | -0.35     |\n",
            "| explained_variance | -1.72e+03 |\n",
            "| fps                | 797       |\n",
            "| nupdates           | 1500      |\n",
            "| policy_entropy     | 0.263     |\n",
            "| total_timesteps    | 7500      |\n",
            "| value_loss         | 0.00583   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.86     |\n",
            "| ep_reward_mean     | -0.17    |\n",
            "| explained_variance | -11.7    |\n",
            "| fps                | 799      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 0.256    |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 0.0117   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.82     |\n",
            "| ep_reward_mean     | -0.13    |\n",
            "| explained_variance | -3.95    |\n",
            "| fps                | 802      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 0.345    |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 0.000154 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.65     |\n",
            "| ep_reward_mean     | -0.14    |\n",
            "| explained_variance | -3.03    |\n",
            "| fps                | 802      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 0.22     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 0.00036  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 5.05     |\n",
            "| ep_reward_mean     | -0.13    |\n",
            "| explained_variance | nan      |\n",
            "| fps                | 804      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 0.421    |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 0.000133 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.94     |\n",
            "| ep_reward_mean     | -0.04    |\n",
            "| explained_variance | nan      |\n",
            "| fps                | 805      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 0.215    |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 1.43e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.92     |\n",
            "| ep_reward_mean     | -0.05    |\n",
            "| explained_variance | -65.8    |\n",
            "| fps                | 806      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 0.189    |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 2.51e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.79     |\n",
            "| ep_reward_mean     | -0.04    |\n",
            "| explained_variance | nan      |\n",
            "| fps                | 807      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 0.193    |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 6.19e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.72     |\n",
            "| ep_reward_mean     | -0.05    |\n",
            "| explained_variance | -160     |\n",
            "| fps                | 808      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 0.31     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 4.68e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.82     |\n",
            "| ep_reward_mean     | -0.03    |\n",
            "| explained_variance | -0.0529  |\n",
            "| fps                | 808      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 0.206    |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 0.000426 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.64     |\n",
            "| ep_reward_mean     | -0.02    |\n",
            "| explained_variance | -24.9    |\n",
            "| fps                | 809      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 0.184    |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 3.64e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.95     |\n",
            "| ep_reward_mean     | -0.05    |\n",
            "| explained_variance | -16      |\n",
            "| fps                | 810      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 0.318    |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 0.000105 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.74     |\n",
            "| ep_reward_mean     | -0.07    |\n",
            "| explained_variance | -6.57    |\n",
            "| fps                | 810      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 0.309    |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 0.000706 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.77     |\n",
            "| ep_reward_mean     | -0.03    |\n",
            "| explained_variance | -1.59    |\n",
            "| fps                | 810      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 0.302    |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 0.000688 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.93     |\n",
            "| ep_reward_mean     | 0        |\n",
            "| explained_variance | -45.1    |\n",
            "| fps                | 812      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 0.315    |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 8.01e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.8      |\n",
            "| ep_reward_mean     | -0.08    |\n",
            "| explained_variance | 0.339    |\n",
            "| fps                | 812      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 0.173    |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 0.000263 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.66     |\n",
            "| ep_reward_mean     | -0.02    |\n",
            "| explained_variance | nan      |\n",
            "| fps                | 813      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 0.167    |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 2.85e-06 |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| ep_len_mean        | 5.1       |\n",
            "| ep_reward_mean     | -0.01     |\n",
            "| explained_variance | -1.74e+04 |\n",
            "| fps                | 814       |\n",
            "| nupdates           | 3200      |\n",
            "| policy_entropy     | 0.178     |\n",
            "| total_timesteps    | 16000     |\n",
            "| value_loss         | 0.000192  |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.82     |\n",
            "| ep_reward_mean     | -0.05    |\n",
            "| explained_variance | -135     |\n",
            "| fps                | 814      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 0.174    |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 0.000131 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.57     |\n",
            "| ep_reward_mean     | -0.04    |\n",
            "| explained_variance | -1.18    |\n",
            "| fps                | 814      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 0.176    |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 0.000156 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.98     |\n",
            "| ep_reward_mean     | -0.1     |\n",
            "| explained_variance | -36.5    |\n",
            "| fps                | 814      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 0.297    |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 5.07e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 5        |\n",
            "| ep_reward_mean     | -0.02    |\n",
            "| explained_variance | -0.324   |\n",
            "| fps                | 815      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 0.281    |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 6.13e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 5.01     |\n",
            "| ep_reward_mean     | -0.05    |\n",
            "| explained_variance | -438     |\n",
            "| fps                | 815      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 0.295    |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 2.12e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.99     |\n",
            "| ep_reward_mean     | 0        |\n",
            "| explained_variance | nan      |\n",
            "| fps                | 815      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 0.163    |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 1.08e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.93     |\n",
            "| ep_reward_mean     | -0.04    |\n",
            "| explained_variance | nan      |\n",
            "| fps                | 816      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 0.155    |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 0.000182 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 5.02     |\n",
            "| ep_reward_mean     | 0        |\n",
            "| explained_variance | -34.5    |\n",
            "| fps                | 816      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 0.165    |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 2.62e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.92     |\n",
            "| ep_reward_mean     | -0.01    |\n",
            "| explained_variance | 0.339    |\n",
            "| fps                | 816      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 0.158    |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 3.01e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.86     |\n",
            "| ep_reward_mean     | -0.03    |\n",
            "| explained_variance | -4.15    |\n",
            "| fps                | 816      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 0.0315   |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 0.000724 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 5        |\n",
            "| ep_reward_mean     | -0.05    |\n",
            "| explained_variance | -6.22    |\n",
            "| fps                | 817      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 0.146    |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 6.5e-05  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 5        |\n",
            "| ep_reward_mean     | -0.04    |\n",
            "| explained_variance | -6.99    |\n",
            "| fps                | 817      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 0.0175   |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 0.000164 |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| ep_len_mean        | 4.93      |\n",
            "| ep_reward_mean     | 0         |\n",
            "| explained_variance | -3.59e+07 |\n",
            "| fps                | 817       |\n",
            "| nupdates           | 4500      |\n",
            "| policy_entropy     | 0.026     |\n",
            "| total_timesteps    | 22500     |\n",
            "| value_loss         | 0.00271   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 5.09     |\n",
            "| ep_reward_mean     | 0        |\n",
            "| explained_variance | -132     |\n",
            "| fps                | 817      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 0.171    |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 4.5e-05  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.56     |\n",
            "| ep_reward_mean     | -0.08    |\n",
            "| explained_variance | -3.92    |\n",
            "| fps                | 818      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 0.288    |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 3.76e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.58     |\n",
            "| ep_reward_mean     | 0        |\n",
            "| explained_variance | -38.7    |\n",
            "| fps                | 818      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 0.166    |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 3.21e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 4.8      |\n",
            "| ep_reward_mean     | 0        |\n",
            "| explained_variance | -8.7e+05 |\n",
            "| fps                | 819      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 0.0336   |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 2.57e-05 |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| ep_len_mean        | 4.77      |\n",
            "| ep_reward_mean     | -0.04     |\n",
            "| explained_variance | -4.04e+06 |\n",
            "| fps                | 819       |\n",
            "| nupdates           | 5000      |\n",
            "| policy_entropy     | 0.141     |\n",
            "| total_timesteps    | 25000     |\n",
            "| value_loss         | 4.35e-05  |\n",
            "----------------------------------\n",
            "Step 1\n",
            "Action:  [0]\n",
            "[[4 0]]  info= [{}]\n",
            "obs= [[4 0]] reward= [0.] done= [False]\n",
            "....x.......\n",
            "Step 2\n",
            "Action:  [1]\n",
            "[[5 2]]  info= [{}]\n",
            "obs= [[5 2]] reward= [0.] done= [False]\n",
            ".....x......\n",
            "Step 3\n",
            "Action:  [1]\n",
            "[[6 0]]  info= [{}]\n",
            "obs= [[6 0]] reward= [0.] done= [False]\n",
            "......x.....\n",
            "Step 4\n",
            "Action:  [0]\n",
            "[[1 1]]  info= [{'episode': {'r': 0, 'l': 4, 't': 31.517486}, 'terminal_observation': array([5, 2])}]\n",
            "Goal reached! reward= [0.] final_state= [5 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H96RMJ7ytS8-"
      },
      "source": [
        "## Ammar's experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTXE9vvQj6CR",
        "outputId": "1f5307ec-b2d7-4e27-804f-e245cdde1657"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "full_train = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "full_test = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# separate out some training data to train the RL agent\n",
        "half_data_size = int(len(trainset)/2)\n",
        "\n",
        "rl_agent_trainset = torch.utils.data.Subset(trainset, range(0,int(0.8*half_data_size)))\n",
        "rl_agent_testset = torch.utils.data.Subset(trainset, range(int(0.8*half_data_size), half_data_size))\n",
        "\n",
        "\n",
        "rl_agent_train = torch.utils.data.DataLoader(rl_agent_trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "rl_agent_test = torch.utils.data.DataLoader(rl_agent_testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Full dataset size:  train={len(full_train)*batch_size} test={len(full_test)*batch_size}\")\n",
        "print(f\"Use a subset of the training data to train the Hyp-RL agent : train={len(rl_agent_train)*batch_size} val={len(rl_agent_test)*batch_size}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Full dataset size:  train=50000 test=10000\n",
            "Use a subset of the training data to train the Hyp-RL agent : train=20000 val=5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD_64TSfk_2N"
      },
      "source": [
        "## Ammar's XCS229ii experiments\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "qG86J6b8rzVA",
        "outputId": "ca043232-47d3-46d7-bbeb-6a78af360c7c"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# function to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting stable-baselines[mpi]==2.10.0\n",
            "  Downloading stable_baselines-2.10.0-py3-none-any.whl (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: mpi4py in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.3.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
            "Installing collected packages: stable-baselines\n",
            "  Attempting uninstall: stable-baselines\n",
            "    Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4ee14a15c1ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# get some random training images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glN6gy1gk_2N",
        "outputId": "eb1df046-6f9c-4cd6-f01c-07d099a56da4"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# separate out some training data to train the RL agent\n",
        "half_data_size = int(len(trainset)/2)\n",
        "\n",
        "rl_agent_trainset = torch.utils.data.Subset(trainset, range(0,int(0.8*half_data_size)))\n",
        "rl_agent_testset = torch.utils.data.Subset(trainset, range(int(0.8*half_data_size), half_data_size))\n",
        "\n",
        "print(f\"Full dataset size:  train={len(trainset)} test={len(testset)}\")\n",
        "print(f\"Use a subset of the training data to train the Hyp-RL agent : train={len(rl_agent_trainset)} val={len(rl_agent_testset)}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Full dataset size:  train=50000 test=10000\n",
            "Use a subset of the training data to train the Hyp-RL agent : train=20000 val=5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGn-0O64lHqh",
        "outputId": "47f450ad-b8d3-4680-f8ab-b36d7d66bfc1"
      },
      "source": [
        "## function to train and evaluate the model given the hyperparameter setting\n",
        "\n",
        "## define the neural network\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def evaluateFullDataset(hp_learning_rate=0.001):\n",
        "  full_train = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "  full_test = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "  net = Net()\n",
        "  loss_criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr=hp_learning_rate, momentum=0.9)\n",
        "  trainAndEvaluateModel(net, loss_criterion, optimizerm, rl_agent_train, rl_agent_test)\n",
        "\n",
        "def trainAndEvaluateModel(net, loss_criterion, optimizer, train, test):\n",
        "  ## Train the model\n",
        "  for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = net(inputs)\n",
        "          loss = loss_criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "              print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 2000))\n",
        "              running_loss = 0.0\n",
        "  print('Finished Training')\n",
        "\n",
        "  ## Test the model\n",
        "  dataiter = iter(test)\n",
        "  images, labels = dataiter.next()\n",
        "\n",
        "  # print images\n",
        "  imshow(torchvision.utils.make_grid(images))\n",
        "  print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n",
        "  outputs = net(images)\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in test:\n",
        "          images, labels = data\n",
        "          # calculate outputs by running images through the network \n",
        "          outputs = net(images)\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Accuracy of the network on the {len(test)} test images: {(100 * correct / total)}%\")\n",
        "  return (100 * correct / total)\n",
        "  \n",
        "evaluateRLAgent()\n",
        "#evaluateFullDataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.001"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALe425tvo_mb"
      },
      "source": [
        "### Build the RL environment and agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t4pXmwpo-tG"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "def evaluateRLAgent(hp_learning_rate=0.001):\n",
        "  #return hp_learning_rate\n",
        "  rl_agent_train = torch.utils.data.DataLoader(rl_agent_trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "  rl_agent_test = torch.utils.data.DataLoader(rl_agent_testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "  net = Net()\n",
        "  loss_criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr=hp_learning_rate, momentum=0.9)\n",
        "  trainAndEvaluateModel(net, loss_criterion, optimizer, rl_agent_train, rl_agent_test)\n",
        "   \n",
        "class TunableHP:\n",
        "  def __init__(self):\n",
        "    self.hyperparameters = {\"learning_rate\":[0.0001, 0.001, 0.01, 0.1, 1.0]}\n",
        "\n",
        "    #self.hyperparameters = {\"learning_rate\":[-5,-4,-3,-2,-1,0,-1,-2,-3,-4,-5]}\n",
        "    self.hyperparameter_keys = list(self.hyperparameters)\n",
        "\n",
        "  def mapStateToHP(self,state):\n",
        "    return [self.hyperparameters[self.hyperparameter_keys[p]][i] for p,i in enumerate(state)]\n",
        "  \n",
        "  def getGridSize(self):\n",
        "    return [len(self.hyperparameters[k]) for k in self.hyperparameter_keys]\n",
        "\n",
        "\n",
        "\n",
        "class HypRLGridEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface.\n",
        "  This is a simple env where the agent must learn to go always left. \n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Define constants for clearer code\n",
        "  UP = 0\n",
        "  DOWN = 1\n",
        "  MAX_ITER = 10\n",
        "\n",
        "  def __init__(self, tunableParams=TunableHP()):\n",
        "    super(HypRLGridEnv, self).__init__()\n",
        "\n",
        "    self.tunableParams = tunableParams\n",
        "\n",
        "    # Size of the grid\n",
        "    self.grid_size = tunableParams.getGridSize()\n",
        "    \n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions, we have two: left and right\n",
        "    n_actions = 2\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    # The observation will be the coordinate of the agent\n",
        "    # this can be described both by Discrete and Box space\n",
        "    self.observation_space = spaces.MultiDiscrete(self.grid_size)\n",
        "\n",
        "    self.eval_cache = np.zeros(self.grid_size)\n",
        "\n",
        "  def eval(self, state):\n",
        "    state = tuple(state)\n",
        "    if self.eval_cache[state] == [0.0]:\n",
        "      # train & test the model for these hyperparameters\n",
        "      self.eval_cache[state] = evaluateRLAgent(*self.tunableParams.mapStateToHP(state))\n",
        "    return self.eval_cache[state]\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # reset the number of iterations for this agent\n",
        "    self.iter = 0\n",
        "    self.reward = -np.inf\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_state = np.random.randint(self.grid_size)\n",
        "\n",
        "    start_state = self.agent_state\n",
        "    self.visited = {}\n",
        "    return np.array(start_state) \n",
        "\n",
        "  def step(self, action):\n",
        "    self.iter += 1\n",
        "\n",
        "    if action == self.UP:\n",
        "      self.agent_state -= 1\n",
        "    elif action == self.DOWN:\n",
        "      self.agent_state += 1\n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "    # Account for the boundaries of the grid\n",
        "    for i, _ in enumerate(self.agent_state):\n",
        "      self.agent_state[i] = np.clip(self.agent_state[i], 0, self.grid_size[i]-1)\n",
        "\n",
        "    # We are done when we visit the same state twice or have taken more iterations than MAX\n",
        "    done = bool(self.iter >= self.MAX_ITER or tuple(self.agent_state) in self.visited)\n",
        "\n",
        "    self.visited[tuple(self.agent_state)] = True\n",
        "\n",
        "    # Reward is minimum of whatever val loss we saw so far\n",
        "    self.reward = max(self.reward, self.eval(self.agent_state))\n",
        "\n",
        "    # Null reward everywhere except when the episode terminates\n",
        "    reward = self.reward if done else 0\n",
        "\n",
        "    # Optionally we can pass additional info, we are not using that for now\n",
        "    info = {}\n",
        "    return np.array(self.agent_state), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # agent is represented as a cross, rest as a dot\n",
        "    print(\".\" * self.agent_state, end=\"\")\n",
        "    print(\"x\", end=\"\")\n",
        "    print(\".\" * (self.grid_size - self.agent_state))\n",
        "\n",
        "  def close(self):\n",
        "    pass\n",
        "\n",
        "# check and make sure the environment is sane and working\n",
        "from stable_baselines.common.env_checker import check_env\n",
        "\n",
        "#env = HypRLGridEnv()\n",
        "# If the environment doesn't follow the interface, an error will be thrown\n",
        "#check_env(env, warn=True)\n",
        "#env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnOq39NCtzbi"
      },
      "source": [
        "### RL Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_gxuc5NtwcW",
        "outputId": "731575ff-dfc7-4537-ed23-98c3a09eac74"
      },
      "source": [
        "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
        "from stable_baselines.common.cmd_util import make_vec_env\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "import pdb\n",
        "# Instantiate the env\n",
        "env = HypRLGridEnv()\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Train the agent\n",
        "#model = ACKTR('MlpPolicy', env, verbose=1).learn(5000)\n",
        "model = A2C(MlpPolicy, env, verbose=1)\n",
        "model.learn(total_timesteps=25000)\n",
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  #pdb.set_trace()\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward, \"final_state=\", info[0]['terminal_observation'])\n",
        "    break\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  #env.render(mode='console')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3sA3g6vgTyz",
        "outputId": "f5cdede7-81a6-44dc-d339-2583574ed53e"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# function to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Requirement already satisfied: stable-baselines[mpi]==2.10.0 in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.5)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: mpi4py in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (3.0.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVaaN7SlgTy1",
        "outputId": "a4cb713d-7529-4c11-d17d-63993546f987"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# separate out some training data to train the RL agent\n",
        "half_data_size = int(len(trainset)/2)\n",
        "\n",
        "rl_agent_trainset = torch.utils.data.Subset(trainset, range(0,int(0.8*half_data_size)))\n",
        "rl_agent_testset = torch.utils.data.Subset(trainset, range(int(0.8*half_data_size), half_data_size))\n",
        "\n",
        "print(f\"Full dataset size:  train={len(trainset)} test={len(testset)}\")\n",
        "print(f\"Use a subset of the training data to train the Hyp-RL agent : train={len(rl_agent_trainset)} val={len(rl_agent_testset)}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Full dataset size:  train=50000 test=10000\n",
            "Use a subset of the training data to train the Hyp-RL agent : train=20000 val=5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3Bv-LgkgTy2"
      },
      "source": [
        "## function to train and evaluate the model given the hyperparameter setting\n",
        "\n",
        "## define the neural network\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def evaluateFullDataset(hp_learning_rate=0.001):\n",
        "  full_train = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "  full_test = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "  net = Net()\n",
        "  loss_criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr=hp_learning_rate, momentum=0.9)\n",
        "  trainAndEvaluateModel(net, loss_criterion, optimizerm, rl_agent_train, rl_agent_test)\n",
        "\n",
        "def trainAndEvaluateModel(net, loss_criterion, optimizer, train, test):\n",
        "  ## Train the model\n",
        "  for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = net(inputs)\n",
        "          loss = loss_criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "              # print('[%d, %5d] loss: %.3f' %\n",
        "              #       (epoch + 1, i + 1, running_loss / 2000))\n",
        "              running_loss = 0.0\n",
        "  #print('Finished Training')\n",
        "\n",
        "  ## Test the model\n",
        "\n",
        "  # # print images\n",
        "  # dataiter = iter(test)\n",
        "  # images, labels = dataiter.next()\n",
        "  # imshow(torchvision.utils.make_grid(images))\n",
        "  # print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "  # outputs = net(images)\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in test:\n",
        "          images, labels = data\n",
        "          # calculate outputs by running images through the network \n",
        "          outputs = net(images)\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Accuracy of the network on the {len(test)} test images: {(100 * correct / total)}%\")\n",
        "  return (100 * correct / total)\n",
        "  \n",
        "\n",
        "#evaluateFullDataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptdvuI8UgTy3"
      },
      "source": [
        "## Build the RL environment and agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU8gYKeHgTy3"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "def evaluateRLAgent(hp_learning_rate):\n",
        "  print(f\"Running evaluation for : {hp_learning_rate}\")\n",
        "  #return hp_learning_rate\n",
        "  rl_agent_train = torch.utils.data.DataLoader(rl_agent_trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "  rl_agent_test = torch.utils.data.DataLoader(rl_agent_testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "  net = Net()\n",
        "  loss_criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr=hp_learning_rate, momentum=0.9)\n",
        "  return trainAndEvaluateModel(net, loss_criterion, optimizer, rl_agent_train, rl_agent_test)\n",
        "   \n",
        "class TunableHP:\n",
        "  def __init__(self):\n",
        "    self.hyperparameters = {\"learning_rate\":[0.0001, 0.001, 0.01, 0.1, 1.0]}\n",
        "\n",
        "    #self.hyperparameters = {\"learning_rate\":[-5,-4,-3,-2,-1,0,-1,-2,-3,-4,-5]}\n",
        "    self.hyperparameter_keys = list(self.hyperparameters)\n",
        "\n",
        "  def mapStateToHP(self,state):\n",
        "    return [self.hyperparameters[self.hyperparameter_keys[p]][i] for p,i in enumerate(state)]\n",
        "  \n",
        "  def getGridSize(self):\n",
        "    return [len(self.hyperparameters[k]) for k in self.hyperparameter_keys]\n",
        "\n",
        "\n",
        "\n",
        "class HypRLGridEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface.\n",
        "  This is a simple env where the agent must learn to go always left. \n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Define constants for clearer code\n",
        "  UP = 0\n",
        "  DOWN = 1\n",
        "  STAY = 2\n",
        "  MAX_ITER = 10\n",
        "\n",
        "  def __init__(self, tunableParams=TunableHP()):\n",
        "    super(HypRLGridEnv, self).__init__()\n",
        "\n",
        "    self.tunableParams = tunableParams\n",
        "\n",
        "    # Size of the grid\n",
        "    self.grid_size = tunableParams.getGridSize()\n",
        "    \n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions, we have two: left and right\n",
        "    n_actions = 3\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    # The observation will be the coordinate of the agent\n",
        "    # this can be described both by Discrete and Box space\n",
        "    self.observation_space = spaces.MultiDiscrete(self.grid_size)\n",
        "\n",
        "    self.eval_cache = np.zeros(self.grid_size)\n",
        "\n",
        "  def eval(self, state):\n",
        "    state = tuple(state)\n",
        "    if self.eval_cache[state] == [0.0]:\n",
        "      # train & test the model for these hyperparameters\n",
        "      self.eval_cache[state] = evaluateRLAgent(*self.tunableParams.mapStateToHP(state))\n",
        "    return self.eval_cache[state]\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # reset the number of iterations for this agent\n",
        "    self.iter = 0\n",
        "    self.reward = -np.inf\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_state = np.random.randint(self.grid_size)\n",
        "\n",
        "    start_state = self.agent_state\n",
        "    self.visited = {}\n",
        "    return np.array(start_state) \n",
        "\n",
        "  def step(self, action):\n",
        "    self.iter += 1\n",
        "\n",
        "    if action == self.UP:\n",
        "      self.agent_state -= 1\n",
        "    elif action == self.DOWN:\n",
        "      self.agent_state += 1\n",
        "    elif action == self.STAY:\n",
        "      self.agent_state = self.agent_state      \n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "    # Account for the boundaries of the grid\n",
        "    for i, _ in enumerate(self.agent_state):\n",
        "      self.agent_state[i] = np.clip(self.agent_state[i], 0, self.grid_size[i]-1)\n",
        "\n",
        "    # We are done when we visit the same state twice or have taken more iterations than MAX\n",
        "    done = bool(self.iter >= self.MAX_ITER or tuple(self.agent_state) in self.visited)\n",
        "\n",
        "    self.visited[tuple(self.agent_state)] = True\n",
        "\n",
        "    # Reward is minimum of whatever val loss we saw so far\n",
        "    self.reward = max(self.reward, self.eval(self.agent_state))\n",
        "\n",
        "    # Null reward everywhere except when the episode terminates\n",
        "    reward = self.reward if done else 0\n",
        "\n",
        "    # Optionally we can pass additional info, we are not using that for now\n",
        "    info = {}\n",
        "    return np.array(self.agent_state), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # agent is represented as a cross, rest as a dot\n",
        "    print(\".\" * self.agent_state, end=\"\")\n",
        "    print(\"x\", end=\"\")\n",
        "    print(\".\" * (self.grid_size - self.agent_state))\n",
        "\n",
        "  def close(self):\n",
        "    pass\n",
        "\n",
        "# check and make sure the environment is sane and working\n",
        "from stable_baselines.common.env_checker import check_env\n",
        "\n",
        "#env = HypRLGridEnv()\n",
        "# If the environment doesn't follow the interface, an error will be thrown\n",
        "#check_env(env, warn=True)\n",
        "#env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1Cn-jklgTy3"
      },
      "source": [
        "### RL Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GBkZ0F8gTy4",
        "outputId": "aa36d1a1-b4ee-4109-ebba-1c1d63e1ed8b"
      },
      "source": [
        "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
        "from stable_baselines.common.cmd_util import make_vec_env\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "import pdb\n",
        "# Instantiate the env\n",
        "env = HypRLGridEnv()\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Train the agent\n",
        "model = ACKTR('MlpPolicy', env, verbose=1).learn(5000)\n",
        "model = A2C(MlpPolicy, env, verbose=1)\n",
        "model.learn(total_timesteps=25000)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/acktr.py:177: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:99: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:298: The name tf.diag is deprecated. Please use tf.linalg.tensor_diag instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:546: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:548: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/acktr.py:307: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/acktr.py:308: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:973: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:914: The name tf.mod is deprecated. Please use tf.math.mod instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:621: The name tf.self_adjoint_eig is deprecated. Please use tf.linalg.eigh instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/acktr.py:320: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Running evaluation for : 0.0001\n",
            "Finished Training\n",
            "Accuracy of the network on the 625 test images: 18.1%\n",
            "Running evaluation for : 1.0\n",
            "Finished Training\n",
            "Accuracy of the network on the 625 test images: 9.62%\n",
            "Running evaluation for : 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 625 test images: 43.22%\n",
            "Running evaluation for : 0.1\n",
            "Finished Training\n",
            "Accuracy of the network on the 625 test images: 9.46%\n",
            "Running evaluation for : 0.01\n",
            "Finished Training\n",
            "Accuracy of the network on the 625 test images: 43.18%\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.43     |\n",
            "| ep_reward_mean     | 25.2     |\n",
            "| explained_variance | -0.00226 |\n",
            "| fps                | 0        |\n",
            "| nupdates           | 1        |\n",
            "| policy_entropy     | 1.1      |\n",
            "| policy_loss        | 26.2     |\n",
            "| total_timesteps    | 20       |\n",
            "| value_loss         | 878      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 3.56     |\n",
            "| ep_reward_mean     | 42.5     |\n",
            "| explained_variance | -0.0238  |\n",
            "| fps                | 8        |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 0.384    |\n",
            "| policy_loss        | 10.1     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 118      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 3.02     |\n",
            "| ep_reward_mean     | 42.9     |\n",
            "| explained_variance | 0.0427   |\n",
            "| fps                | 17       |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 0.0468   |\n",
            "| policy_loss        | 0.0449   |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 31.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.5      |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.101    |\n",
            "| fps                | 21       |\n",
            "| nupdates           | 1        |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 5        |\n",
            "| value_loss         | 1.85e+03 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.65     |\n",
            "| ep_reward_mean     | 29.8     |\n",
            "| explained_variance | -0.00297 |\n",
            "| fps                | 521      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 1.1e+03  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.68     |\n",
            "| ep_reward_mean     | 27.8     |\n",
            "| explained_variance | -0.0909  |\n",
            "| fps                | 601      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.83e+03 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.66     |\n",
            "| ep_reward_mean     | 30.8     |\n",
            "| explained_variance | -0.00709 |\n",
            "| fps                | 618      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 1.09e+03 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.73     |\n",
            "| ep_reward_mean     | 28.5     |\n",
            "| explained_variance | 0.000636 |\n",
            "| fps                | 634      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 1.43e+03 |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| ep_len_mean        | 2.59      |\n",
            "| ep_reward_mean     | 28.3      |\n",
            "| explained_variance | -0.000667 |\n",
            "| fps                | 647       |\n",
            "| nupdates           | 500       |\n",
            "| policy_entropy     | 1.1       |\n",
            "| total_timesteps    | 2500      |\n",
            "| value_loss         | 1.37e+03  |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.78     |\n",
            "| ep_reward_mean     | 30.5     |\n",
            "| explained_variance | -0.00273 |\n",
            "| fps                | 657      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 1.09e+03 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.75     |\n",
            "| ep_reward_mean     | 30.9     |\n",
            "| explained_variance | 0.00449  |\n",
            "| fps                | 664      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 617      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.73     |\n",
            "| ep_reward_mean     | 30.5     |\n",
            "| explained_variance | -0.0121  |\n",
            "| fps                | 671      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 22.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.79     |\n",
            "| ep_reward_mean     | 29.4     |\n",
            "| explained_variance | -0.0364  |\n",
            "| fps                | 674      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 559      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.7      |\n",
            "| ep_reward_mean     | 31.4     |\n",
            "| explained_variance | 0.108    |\n",
            "| fps                | 678      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 1.08     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 384      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.7      |\n",
            "| ep_reward_mean     | 32.2     |\n",
            "| explained_variance | -0.0674  |\n",
            "| fps                | 680      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 1.06     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 268      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.84     |\n",
            "| ep_reward_mean     | 34.6     |\n",
            "| explained_variance | -0.0523  |\n",
            "| fps                | 681      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 0.938    |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 193      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.87     |\n",
            "| ep_reward_mean     | 39       |\n",
            "| explained_variance | -0.369   |\n",
            "| fps                | 683      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 0.796    |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 132      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.82     |\n",
            "| ep_reward_mean     | 42.5     |\n",
            "| explained_variance | 0.0574   |\n",
            "| fps                | 685      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 0.794    |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 34       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.9      |\n",
            "| ep_reward_mean     | 42       |\n",
            "| explained_variance | 0.0988   |\n",
            "| fps                | 686      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 0.882    |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 4.9      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.42     |\n",
            "| ep_reward_mean     | 42.9     |\n",
            "| explained_variance | 0.663    |\n",
            "| fps                | 685      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 0.44     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 0.0203   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.37     |\n",
            "| ep_reward_mean     | 41.9     |\n",
            "| explained_variance | 0.0436   |\n",
            "| fps                | 685      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 0.256    |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 426      |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| ep_len_mean        | 2.29      |\n",
            "| ep_reward_mean     | 42.9      |\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 686       |\n",
            "| nupdates           | 1800      |\n",
            "| policy_entropy     | 0.0949    |\n",
            "| total_timesteps    | 9000      |\n",
            "| value_loss         | 0.0478    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.21     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.969    |\n",
            "| fps                | 684      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 0.0794   |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 0.00728  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.13     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.756    |\n",
            "| fps                | 685      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 0.0528   |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 0.0332   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.27     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.375    |\n",
            "| fps                | 683      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 0.0598   |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 0.0375   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.23     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.376    |\n",
            "| fps                | 682      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 0.0489   |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 0.0287   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.14     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.724    |\n",
            "| fps                | 683      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 0.0465   |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 0.0527   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.23     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.000265 |\n",
            "| fps                | 684      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 0.017    |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 0.0524   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.25     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.372    |\n",
            "| fps                | 685      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 0.0171   |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 0.0362   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.22     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.986    |\n",
            "| fps                | 685      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 0.0153   |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 0.00156  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.21     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.946    |\n",
            "| fps                | 683      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 0.0254   |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 0.0214   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.23     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.371    |\n",
            "| fps                | 684      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 0.0124   |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 0.0293   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.17     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.974    |\n",
            "| fps                | 684      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 0.0243   |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 0.0061   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.18     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.755    |\n",
            "| fps                | 685      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 0.0172   |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 0.0282   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.3      |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.784    |\n",
            "| fps                | 686      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 0.0152   |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 0.0258   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.15     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.496    |\n",
            "| fps                | 685      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 0.0185   |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 0.0295   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.3      |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.947    |\n",
            "| fps                | 684      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 0.0175   |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 0.00712  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.2      |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.942    |\n",
            "| fps                | 682      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 0.0108   |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 0.00635  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.2      |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0        |\n",
            "| fps                | 680      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 0.0116   |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 0.0798   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.17     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.438    |\n",
            "| fps                | 679      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 0.0112   |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 0.0475   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.24     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.277    |\n",
            "| fps                | 677      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 0.0184   |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 0.0408   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.21     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.952    |\n",
            "| fps                | 675      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 0.0176   |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 0.0116   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.23     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.945    |\n",
            "| fps                | 674      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 0.0177   |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 0.0374   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.23     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.307    |\n",
            "| fps                | 672      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 0.0179   |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 0.0326   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.22     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.375    |\n",
            "| fps                | 670      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 0.0183   |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 0.0396   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.22     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.779    |\n",
            "| fps                | 667      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 0.0132   |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 0.0245   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.19     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.374    |\n",
            "| fps                | 665      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 0.0158   |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 0.0319   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.15     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.538    |\n",
            "| fps                | 664      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 0.00788  |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 0.0276   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.21     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.524    |\n",
            "| fps                | 664      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 0.0158   |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 0.0277   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.22     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.948    |\n",
            "| fps                | 663      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 0.0127   |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 0.0156   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.13     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 662      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 0.0131   |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 0.0453   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.18     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.788    |\n",
            "| fps                | 661      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 0.0124   |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 0.0232   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.19     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.52     |\n",
            "| fps                | 662      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 0.0109   |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 0.0271   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.24     |\n",
            "| ep_reward_mean     | 43.2     |\n",
            "| explained_variance | 0.983    |\n",
            "| fps                | 662      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 0.00981  |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 0.00566  |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines.a2c.a2c.A2C at 0x7fabaa19dad0>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb0mvA28dKRz",
        "outputId": "9f35a4da-e441-4a8a-823c-1c76cc296b53"
      },
      "source": [
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  #pdb.set_trace()\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward, \"final_state=\", info[0]['terminal_observation'])\n",
        "    break\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  #env.render(mode='console')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Action:  [2]\n",
            "obs= [[1]] reward= [0.] done= [False]\n",
            "Step 2\n",
            "Action:  [2]\n",
            "Goal reached! reward= [43.22] final_state= [1]\n"
          ]
        }
      ]
    }
  ]
}